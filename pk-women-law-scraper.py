import requests
from bs4 import BeautifulSoup
import re
import os
import time
from googlesearch import search
import PyPDF2

# Create directory for PDFs
current_dir = os.path.dirname(os.path.abspath(__file__))
pdf_dir = os.path.join(current_dir, 'women_laws_pdfs')

if not os.path.exists(pdf_dir):
    os.makedirs(pdf_dir)

def extract_federal_laws(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    def extract_nested_text(element, level=0):
        laws = []
        if element.name in ['li', 'p', 'div']:
            text = element.get_text(separator=' ', strip=True)
            if any(k in text for k in ['Act', 'Ordinance', 'Law', 'Bill', 'Amendment']) and len(text) > 5:
                laws.append(('  ' * level) + text)

        for child in element.find_all(['ul', 'ol', 'li', 'div'], recursive=False):
            laws.extend(extract_nested_text(child, level + 1))

        return laws

    try:
        print(f"Fetching content from {url}...")
        response = requests.get(url, headers=headers)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        main_heading = None
        for h in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            text = h.get_text().strip()
            if "Federal" in text and ("Laws" in text or "Law" in text):
                main_heading = h
                break

        federal_laws = []

        if main_heading:
            current_element = main_heading.find_next_sibling()
            while current_element:
                if current_element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    if any(province in current_element.get_text().lower() for province in ['punjab', 'sindh', 'sind', 'balochistan', 'khyber', 'gilgit']):
                        break
                if current_element.name in ['ul', 'ol', 'div']:
                    federal_laws.extend(extract_nested_text(current_element))
                current_element = current_element.find_next_sibling()
        else:
            print("No heading found. Falling back to generic list search...")
            for container in soup.find_all(['ul', 'ol', 'div']):
                federal_laws.extend(extract_nested_text(container))


        cleaned_laws = []
        valid_keywords = ['act', 'law', 'bill', 'ordinance', 'amendment']

        for law in federal_laws:
            cleaned_law = re.sub(r'^[\d\.\-\•\s]+', '', law).strip()

            if cleaned_law and len(cleaned_law) > 4 and not any(p in cleaned_law.lower() for p in ['punjab', 'sindh', 'balochistan', 'khyber', 'gilgit']):
                if any(keyword in cleaned_law.lower() for keyword in valid_keywords):
                    cleaned_laws.append(cleaned_law)

        cleaned_laws = list(dict.fromkeys(cleaned_laws))

        return cleaned_laws

    except Exception as e:
        print(f"Error extracting laws: {e}")
        return []


def search_law_pdfs(laws):
    results = {}
    
    for law in laws:
        print(f"Searching for PDF of: {law}")
        
        search_queries = [
            f"{law} Pakistan official pdf filetype:pdf",
            f"{law} pakistan government gazette filetype:pdf",
            f"{law} text pakistan filetype:pdf",
            f"{law} act pakistan filetype:pdf",
            f"pakistan {law} official document filetype:pdf"
        ]
        
        pdf_urls = []
        for query in search_queries:
            try:
                print(f"  Trying query: {query}")
                
                for url in search(query, num=3, stop=3, pause=2):
                    if url.lower().endswith('.pdf') and url not in pdf_urls:
                        pdf_urls.append(url)
                
                if pdf_urls:
                    print(f"  Found {len(pdf_urls)} potential PDFs")
                    break 
                
                time.sleep(3)
                
            except Exception as e:
                print(f"  Error with search query '{query}': {e}")
                time.sleep(5)
        
        if pdf_urls:
            success = False
            for pdf_url in pdf_urls:
                try:
                    pdf_name = re.sub(r'[^\w\s-]', '', law)
                    pdf_name = re.sub(r'\s+', '_', pdf_name) 
                    pdf_name = pdf_name[:50] + ".pdf"
                    pdf_path = os.path.join('pro_women_laws_pdfs', pdf_name)
                    
                    pdf_response = requests.get(pdf_url, stream=True, timeout=30)
                    pdf_response.raise_for_status()
                    
                    if 'application/pdf' in pdf_response.headers.get('Content-Type', ''):
                        with open(pdf_path, 'wb') as f:
                            for chunk in pdf_response.iter_content(chunk_size=8192):
                                f.write(chunk)
                        
                        print(f"  Successfully downloaded PDF for '{law}' to {pdf_path}")
                        results[law] = {
                            'status': 'Success',
                            'url': pdf_url,
                            'path': pdf_path
                        }
                        success = True
                        break
                    else:
                        print(f"  URL {pdf_url} is not a valid PDF despite .pdf extension")
                        
                except Exception as e:
                    print(f"  Error downloading PDF from {pdf_url}: {e}")
            
            if not success:
                results[law] = {
                    'status': 'Found URLs but download failed',
                    'urls': pdf_urls
                }
        else:
            print(f"  No PDF found for '{law}'")
            results[law] = {
                'status': 'No PDF found'
            }
                
        time.sleep(5)
    
    return results

# Function to verify PDFs and check their content
def verify_pdfs(results):
    for law, result in results.items():
        if result['status'] == 'Success' and 'path' in result:
            pdf_path = result['path']
            try:
                with open(pdf_path, 'rb') as f:
                    pdf_reader = PyPDF2.PdfReader(f)
                    num_pages = len(pdf_reader.pages)
                    
                    first_page_text = pdf_reader.pages[0].extract_text()
                    
                    law_keywords = [word.lower() for word in law.split() if len(word) > 3]
                    matches = sum(1 for keyword in law_keywords if keyword in first_page_text.lower())
                    relevance_score = matches / len(law_keywords) if law_keywords else 0
                    
                    result['verification'] = {
                        'valid_pdf': True,
                        'pages': num_pages,
                        'relevance_score': relevance_score,
                        'likely_match': relevance_score > 0.3
                    }
                    
                    print(f"Verified PDF for '{law}': {num_pages} pages, relevance score: {relevance_score:.2f}")
                    
            except Exception as e:
                print(f"Error verifying PDF for '{law}': {e}")
                result['verification'] = {
                    'valid_pdf': False,
                    'error': str(e)
                }
    
    return results

# Function to analyze a specific PDF
def analyze_pdf(pdf_path, pages_to_analyze=3):
    try:
        with open(pdf_path, 'rb') as f:
            pdf_reader = PyPDF2.PdfReader(f)
            num_pages = len(pdf_reader.pages)
            
            print(f"PDF has {num_pages} pages")
            
            pages_to_read = min(pages_to_analyze, num_pages)
            for i in range(pages_to_read):
                page_text = pdf_reader.pages[i].extract_text()
                
                preview = page_text[:500] + "..." if len(page_text) > 500 else page_text
                print(f"\nPage {i+1} preview:\n{preview}")
    except Exception as e:
        print(f"Error analyzing PDF: {e}")

# Function to try direct DOM exploration as a last resort
def explore_dom_for_laws(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    def extract_nested_text(element, level=0):
        laws = []
        if element.name in ['p', 'div', 'li']:
            text = element.get_text(separator=' ', strip=True)
            if any(k in text for k in ['Act', 'Ordinance', 'Law', 'Bill', 'Amendment']) and len(text) > 15:
                laws.append(('  ' * level) + text)

        for child in element.find_all(['ul', 'ol', 'li', 'div'], recursive=False):
            laws.extend(extract_nested_text(child, level + 1))

        return laws

    try:
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        potential_laws = []

        for container in soup.find_all(['div', 'p', 'ul', 'ol']):
            potential_laws.extend(extract_nested_text(container))

        cleaned_laws = []
        valid_keywords = ['act', 'law', 'bill', 'ordinance', 'amendment']

        for law in potential_laws:
            cleaned_law = re.sub(r'^[\d\.\-\•\s]+', '', law).strip()

            if cleaned_law and len(cleaned_law) > 4 and not any(p in cleaned_law.lower() for p in ['punjab', 'sindh', 'balochistan', 'khyber', 'gilgit', 'sind']):
                if any(keyword in cleaned_law.lower() for keyword in valid_keywords):
                    cleaned_laws.append(cleaned_law)

        cleaned_laws = list(dict.fromkeys(cleaned_laws))

        return cleaned_laws

    except Exception as e:
        print(f"Error in DOM exploration: {e}")
        return []


def main():
    url = "https://ncsw.gov.pk/Detail/OWYxM2U1MWYtZDZhNS00YTA3LWIwOTItNzBhMmZlMDIxNzlj"
    
    print("\nExtracting federal laws from the website...")
    federal_laws = extract_federal_laws(url)
    
    if not federal_laws:
        print("\nPrimary extraction method failed. Trying alternative approach...")
        federal_laws = explore_dom_for_laws(url)
    
    if not federal_laws:
        print("\nCould not extract any laws from the webpage. Please check the webpage structure manually.")
        print("You can examine the saved 'webpage_content.html' file to identify the laws.")
        return
    
    print("\nFound the following federal laws:")
    for i, law in enumerate(federal_laws, 1):
        print(f"{i}. {law}")
    
    print("\nNow searching for PDF versions of these laws...")
    results = search_law_pdfs(federal_laws)
    
    print("\nVerifying downloaded PDFs...")
    try:
        verified_results = verify_pdfs(results)
    except Exception as e:
        print(f"Error during PDF verification: {e}")
        verified_results = results
    
    print("\nSummary of results:")
    for law, result in verified_results.items():
        print(f"\n- {law}")
        print(f"  Status: {result['status']}")
        if 'url' in result:
            print(f"  URL: {result['url']}")
        if 'path' in result:
            print(f"  Saved to: {result['path']}")
        if 'verification' in result:
            v = result['verification']
            if v.get('valid_pdf', False):
                print(f"  Pages: {v['pages']}")
                print(f"  Relevance score: {v['relevance_score']:.2f}")
                print(f"  Likely match: {'Yes' if v.get('likely_match', False) else 'No'}")

if __name__ == "__main__":
    main()